# Introducción {#introduccion}

\unchapter{Antecedentes}
\begin{textblock*}{15cm}(3cm,2cm) 
\begin{center}
\rule{8cm}{.005cm}\\*
\vspace{.5cm}
\textbf{{\fontsize{25}{23}\selectfont 1. Antecedentes}}\\
\rule{8cm}{.005cm}\\*
\end{center}
\end{textblock*}
\[\]
\section{Introducción}

\thispagestyle{empty}
Los procesos de adquisición de conocimiento son diversos, pueden ser extremadamente complejos, e involucran una variedad de mecanismos que se combinan y complementan. Un elemento básico y fundamental del aprendizaje, sin embargo, es la \azulcursiva{observación}. Desde los tiempos mas remotos, los seres humanos han acumulado conocimiento observando su entorno y, en particular, constatando que algunos eventos se repiten sin cambios cuando se observan los fenómenos de interés.
\par
De esta manera, la reiteración de patrones y tendencias ha conducido a sugerir, y en ocasiones a dar por ciertas e inmutables, algunas \azulcursiva{relaciones de causa-efecto}. Así, se han establecido hechos que pasan a formar parte del enorme acervo de conocimiento común. 
\par
Con este principio nuestros ancestros aprendieron que la noche sigue al día y que la falta de agua acaba con los cultivos, por ejemplo. Supieron también de las propiedades de algunos materiales, de los efectos del fuego y de la toxicidad de algunas fuentes de alimento, entre muchos otros ejemplos. La \azulcursiva{causalidad determinística} (a una causa corresponde siempre un mismo efecto) subyace en esta forma de aprendizaje.
\par
Por supuesto, existe una gran diferencia entre saber que, en determinadas condiciones, siempre se produce el mismo efecto y, en otro nivel, conocer el mecanismo por el cual la causa conduce a ese efecto. Es necesario acceder a otro nivel cognitivo para resolver la segunda cuestión. Este paso equivale a transformar el conocimiento común en lo que se denomina \azulcursiva{conocimiento científico} y debieron transcurrir muchos años antes de que la \azulcursiva{Ciencia} se estableciera y desarrollase teorías, modelos y mecanismos para que esa transformación pudiera tener lugar.
\par
Ahora bien, existe otra clase de fenómenos, más abundantes que los determinísticos y, por tal razón, muy importantes en lo que se refiere al aprendizaje y la adquisición de conocimiento. Estos fenómenos, aun cuando sean observados en las mismas condiciones, no necesariamente producen los mismos resultados. A una misma causa ya no corresponde un efecto único sino toda una colección de efectos posibles. Es mucho más difícil aprender de estos \azulcursiva{fenómenos inciertos} que de los determinísticos. \azulcursiva{La variabilidad} de sus resultados produce \azulcursiva{incertidumbre} y, durante mucho tiempo, los seres humanos enfrentaron ese desasosiego con argumentos que incluían la magia, la fe y la resignación. 
\par
Los ejemplos de fenómenos inciertos surgen lo mismo en el más primitivo ambiente natural que en el sofisticado entorno de la sociedad actual. A un día lluvioso, no necesariamente le seguirá otro en que llueva también. La cantidad de frutos que produce un naranjo cambia de cosecha a cosecha así se cultive con los mismos cuidados. La aplicación de un tratamiento medico no necesariamente tiene el mismo efecto en todos los pacientes y un usuario activo de las redes sociales no reacciona igual que todos los demás frente a un suceso publico. La lista es interminable.
\par
Una discusión que reviste gran interés es la que procura identificar la fuente de la incertidumbre. ¿Es inherente al fenómeno? ¿Acaso es fruto de la ignorancia del observador, que supone que la observación se produce en las mismas condiciones cuando no es así? La materia da para disquisiciones que actualmente siguen su curso en el campo de la Filosofía. El hecho, es que los fenómenos inciertos existen, se materializan en el entorno cotidiano y, más aun, sus efectos tienen impacto en la vida de los seres humanos. Como consecuencia, y sin dejar de lado el interés por la génesis de la incertidumbre, es necesario contar con herramientas que permitan aprender y obtener conocimiento de esta clase de fenómenos.
\par
La observación repetida, en el caso determinístico, da paso gradualmente a la certeza de la causalidad y, como ya se ha argumentado, establece la base para el conocimiento común. Esto no ocurre de la misma manera cuando hay multiplicidad de efectos posibles. La observación repetida revela y confirma la \azulcursiva{variabilidad} y, por tanto, la incertidumbre. El conocimiento común es de horizontes muy limitados en esas condiciones.
\par
Es natural que incluso las ideas mas elementales que se utilizan para extraer conocimiento de los fenómenos inciertos sean mas elaboradas que sus contrapartes en un ambiente determinístico y que se hayan desarrollado mucho después, a medida que la noción de conocimiento científico se iba consolidando. De hecho, la \azulcursiva{Estadística}, de la que nos ocuparemos en este curso, es una disciplina de carácter científico y juega un papel fundamental en el estudio de fenómenos inciertos  provenientes de las mas variadas áreas de conocimiento. Como otras especialidades de la ciencia, la Estadística cuenta con sus propios conceptos, principios, métodos y técnicas y, cuando se entrevera con el estudio de otros campos, da lugar a ramificaciones diversas (Bioestadística, Econometría, Geoestadística y Astroestadística, por ejemplo) que tienen su propio desarrollo, pero retroalimentan al cuerpo principal de conocimiento estadístico.
\par
En una perspectiva histórica, algunos autores ubican el origen de la Estadística en el siglo XVII, con el trabajo de \azulcursiva{John Graunt} (1662) que se ocupo del estudio de la mortalidad en Londres; en particular, del análisis de la edad al morir. A partir de una considerable cantidad de registros parroquiales, propuso la sistematización y el \azulcursiva{resumen} de la información a través de una tabla de mortalidad, que permite entender algunos de los aspectos más relevantes de este fenómeno. Desde entonces y hasta finales del siglo XIX, el problema de producir conocimiento, a partir de observaciones provenientes de fenómenos inciertos, ha sido objeto de distintos tratamientos y aproximaciones por parte de una variedad de personajes que ocupan lugares prominentes en el desarrollo de la Ciencia. \azulcursiva{J. Bernoulli}(1655-1705), \azulcursiva{de Moivre} (1667-1754), \azulcursiva{D. Bernoulli} (1700-1782),\azulcursiva{Bayes} (1702-1761),\azulcursiva{Laplace} (1749-1827),\azulcursiva{ Legendre }(1752-1833), \azulcursiva{Gauss} (1777-1855), \azulcursiva{Quetelet} (1796-1874), \azulcursiva{Chebyshev}(1821-1894), \azulcursiva{Galton} (1822-1911) y \azulcursiva{Pearson} (1857-1936) son algunos de ellos.
\par
En esta lista, con la probable excepción de \azulcursiva{Pearson}, ninguno se hace llamar Estadístico. Mas aun, en esa época, los protagonistas del desarrollo de los métodos estadísticos se ocupaban tanto del estudio de fenómenos determinísticos como inciertos y provenían de diversos orígenes profesiones como Abogacía, Medicina, Física, Química, Astronomía, Ingeniera, Biología, Geografía o Sociología.
\par
Los intereses de estos precursores incluían el estudio de la mecánica celeste, la propagación de las epidemias, la valoración de la evidencia en los juicios, los procesos de la herencia biológica, la efectividad de los fertilizantes en lo cultivos e incluso los precios de los seguros, entre muchos otros más. En su trabajo, un elemento común era el interés en las Matemáticas, una herramienta fundamental para describir los fenómenos de interés.
\par
A lo largo de esos años, la observación, la medición y la cuantificación de los fenómenos, alcanzó un papel central en el proceso de aprendizaje; especialmente, como parte del mecanismo que permite contrastar modelos y teorías con la realidad que pretenden describir. Este es un elemento clave de lo que se habría de conocer como el \azulcursiva{Método Científico}.
\par
En el caso de los fenómenos inciertos, parte fundamental del énfasis se colocó en la \azulcursiva{medición de la incertidumbre}. En el proceso, se estableció una primera conexión con el cálculo de probabilidades que ya habían empezado a formalizar Pascal (1623-1662) y Fermat (1607-1665), entre otros. 
\par
\azulcursiva{La Probabilidad, una medida de la incertidumbre}, se había desarrollado sobre todo en relación a los juegos de azar. La razón para elegir ese objeto de estudio es, en la actualidad, es muy interesante desde el punto de vista pedagógico. Cada juego suele tener asociada una colección finita de resultados elementales (los efectos) que cuentan con las mismas posibilidades de ocurrir.
\par
Estos resultados son equiprobables, de manera que la asignación de probabilidad para eventos más complejos se puede realizar calculando el cociente de casos favorables entre casos posibles. Este procedimiento reduce el problema de asignar probabilidades al de enumerar los resultados que hacen posible un determinado evento y, en su momento, dio un gran impulso a lo que ahora se denomina el análisis combinatorio. En estricto sentido, la asignación clásica de probabilidades, en virtud del supuesto de \azulcursiva{equiprobabilidad}, no requiere la observación del fenómeno. 
\par
Todo el análisis es deductivo y se basa en la razón. Esta herramienta para la medición de la incertidumbre resultó exitosa y fructífera, con diversas ramificaciones. Estas, sin embargo, no fueron susceptibles de aplicarse a la descripción de los fenómenos naturales o sociales en general.
\par
Entre los obstáculos que se enfrentaron destacan dos, de naturaleza distinta. Por una parte, en el nivel conceptual, utilizar la probabilidad para describir la variabilidad de las observaciones que provienen de los fenómenos naturales presupondría que estas se producen aleatoriamente, con un mecanismo como el que da lugar a los resultados de los juegos de azar. Por otra parte, también sería indispensable suponer que los resultados elementales de estos fenómenos son equiprobables. El primer supuesto dio lugar a notables discusiones con posiciones diferentes y encontradas al respecto. Por su parte, la hipótesis de probabilidades iguales no solo ha resultado insostenible en muchas aplicaciones, sino que, de cumplirse, paradójicamente haría innecesaria la observación del fenómeno.
\par
Cuando el estudio de la probabilidad abandonó el terreno del racionalismo puro y adoptó un enfoque mas empírico, fue posible considerar fenómenos con resultados no equiprobables. Bajo esta concepción, la probabilidad de un evento A, que puede ocurrir cuando se realiza un experimento, se define como el límite de la frecuencia relativa con que ocurre A, cuando el número de veces que se realiza el experimento (en las mismas condiciones) tiende a infinito.
\par
Así, el cociente casos posibles entre el número total de casos dio paso al cociente del número de observaciones exitosas entre el total de observaciones; el supuesto de probabilidades iguales se reemplazó por el de la observabilidad del fenómeno, en las mismas condiciones, una cantidad ilimitada de ocasiones. Esta modificación dejó en claro que el estudio de los fenómenos inciertos se puede llevar a cabo a partir de las observaciones que produce y tiene naturaleza inductiva (de la información particular provista por la observación, a las características generales del fenómeno).
\par
En este punto, si bien no forma parte del temario de este curso (se revisa en el de \azulcursiva{Estadística Bayesiana}), es importante recordar que entre las alternativas para el estudio de los fenómenos inciertos que surgieron antes del Siglo XX, ocupó un lugar destacado la idea de la \azulcursiva{Probabilidad Inversa}. Durante los Siglos XVIII y XIX, esta noción cobró creciente importancia. Primero con \azulcursiva{Bayes}, pero especialmente con\azulcursiva{ Laplace}, se estableció esta línea de pensamiento que, retomando la relación causa-efecto, propone que, en el caso de los fenómenos inciertos, una descripción probabilística de los efectos posibles cuando se conoce la causa, combinada con una descripción de la incertidumbre inicial respecto a la causa desconocida, podía dar lugar a una descripción actualizada de la incertidumbre sobre la causa, dada una colección de efectos (datos) observados.
\par
Ese enfoque establece un mecanismo para llevar a cabo este proceso de aprendizaje. Este se reduce a la, ahora muy conocida, formula de Bayes. Citando al propio Laplace (1774):
\begin{center}
\azulcursiva{âIf an event can be produced by a number k of different causes, the
probabilities of these causes given the event are to each other as the
probabilities of the event given the causes".}
\end{center}
Esta aproximación al estudio de los fenómenos inciertos fue desplazada, al filo del Siglo XX, por las  ideas de la Estadística que vamos a revisar en este curso y que se conocen, en su conjunto, como Estadística Frecuentista o Estadística Matemática. Se trata de la escuela de pensamiento dominante durante la mayor parte del siglo pasado; ha sido extraordinariamente eficaz y reúne una variedad de ideas brillantes que surgen de los diferentes problemas específicos que aborda. Establece criterios precisos para evaluar la optimalidad de sus procedimientos y se desarrolla con rigor matemático. En la práctica ha sido muy exitoso, en particular debido a que sus métodos están al alcance de los usuarios a través de una gran cantidad de aplicaciones de cómputo. Por otra parte, es conveniente reconocer que este enfoque también tiene, como es natural, algunas limitaciones que se señalarán oportunamente.
\section{La Naturaleza del Análisis Estadístico}
La Estadística es una disciplina científica que sirve para aprender de los fenómenos inciertos. En prácticamente todos los textos de Estadística se incluye una descripción de lo que los distintos autores entienden por Estadística. Lo mas curioso, quizá, es que sus descripciones tienen un grado de generalidad y una falta de precisión que no corresponden con un texto de aspiraciones científicas. Veamos algunos casos:
\begin{itemize}  
\item{R.A. Fisher (1925/1970): \azulcursiva{âThe science of statistics is essentially a branch of Applied Mathematics and may be regarded as mathematics applied to observational data".}}
\item{ G.W. Snedecor y W.G. Cochran (1937/1967):\azulcursiva{â...Statistics deals with techniques for collecting, analyzing, and drawing conclusions from 
data".}}
\item{Mood, A.M., Graybill, F.A. y Boes, D.C. (1974):\azulcursiva{âOne function of Statistics is the provision of techniques for making inductive inference and for measuring the degree of uncertainty of such inferences".}}
\end{itemize}
\par
Estas descripciones, lo mismo que las que parecen en muchos otros textos, con diferentes grados de especificidad, proveen elementos para familiarizarse con la disciplina. Dan cuenta de como podría ser considerada, establecen que su materia prima son los datos. Señalan una función de la Estadística. Sin embargo, no la caracterizan, como sería de esperarse de una definición formal. Esta falta de precisión no deja de ser peculiar, especialmente si se toma en cuenta, por un lado, el rigor matemático con que estos textos desarrollan su contenido y, por otra parte, el tiempo que ha transcurrido desde que la Estadística se estableció como un instrumento fundamental para la adquisición de conocimiento científico. De cualquier forma, sea cual sea la causa de esta anomalía, en este curso adoptaremos una definición, formal y precisa, a la que recurriremos constantemente para contrastar las ideas centrales en el desarrollo de los distintos temas:
\begin{definition}
\azulcursiva{La Estadística es una disciplina cuyo propósito es \textbf{describir} fenómenos que se manifiestan a través de datos que presentan \textbf{variabilidad.}}
\end{definition}
\par
Esta caracterización delimita, puntualmente, el ámbito de acción âlos fenómenos que presentan \textit{variabilidad}- y, al mismo tiempo, establece el objetivo que se persigue:\textit{la descripción}.
\par
De esta forma, un procedimiento para el análisis de datos se puede identificar como parte del acervo estadístico si,
\begin{itemize}
\item Su objetivo es la \azulcursiva{descripción} del fenómeno que originó los datos,
\item Los datos con que realiza el análisis presentan \azulcursiva{variabilidad}.
\par
\end{itemize}
De esta forma, los dos términos clave son \azulcursiva{descripción} y \azulcursiva{variabilidad}. Si se cae en la tentación del \textit{glamour}, se puede sustituir la palabra disciplina en la definición por la de \textit{Ciencia}. En el otro extremo, ni ciencia, ni disciplina, sino simplemente un conjunto de técnicas. Una consecuencia de esta definición, es que \azulcursiva{toda la Estadística es descriptiva}, idea que se contrapone con la división que algunos viejos textos en la materia establecen cuando separan lo que ellos denominan Estadística Descriptiva de la Inferencia Estadística. La definición es general y aplica para todos las componentes de la disciplina. Antes de iniciar la presentación de las técnicas de que se sirve la Estadística para cumplir su cometido, es pertinente presentar algunas ideas generales
\par
Cuando se aborda el estudio de un fenómeno, habitualmente esta tarea se acomete con un objetivo específico y con información previa que puede proceder de diversas fuentes. Estos dos elementos determinan el Marco de Referencia de la investigación. 
\par
El tema es relevante porque un fenómeno se puede manifestar en una variedad de formas distintas, pero es precisamente el marco de referencia el que establece los atributos que son de interés en un estudio particular
\par
Como ejemplo, considere un estudio con el que interesa describir el sistema de salud público del país. Antes de realizar la investigación es muy probable que exista información sobre algunos temas relacionados, proveniente de diversas fuentes y con distintos niveles de calidad. En cuanto a los objetivos, estos pueden ser de naturaleza muy variada. Si el estudio lo patrocina un proveedor internacional de suministros médicos, es posible que el interés se concentre en las oportunidades de negocio que se puedan concretar con el sector. En otro extremo, si el proyecto lo desarrolla una institución académica de investigación, el objetivo podría estar ligado a la eficiencia con que el sector atiende las necesidades de la población.
\par
Así, en el caso del proveedor, el atributo de interés sería el \textit{Potencial de Negocio}, mientras que para la investigación académica, el atributo que interesa es la \textit{Eficiencia del Sector}. 
\begin{definition}
\azulcursiva{En general, se define un Atributo de Interés como un
rasgo a través del que se busca describir el fenómeno bajo estudio.}
\end{definition}
\par
Los atributos son de tipos muy variados y pueden cambiar de un estudio a otro, lo mismo que en una misma investigación, a lo largo del tiempo. Con frecuencia, como en los ejemplos del Potencial y la Eficiencia, se definen en términos conceptuales. Pero aun cuando se refieran a aspectos en apariencia más tangibles, es necesario precisar con detalle la forma en que serán registrados en el estudio. Suponga que en el estudio del sector salud, el atributo de interés es el \textit{Tamaño del sector}. Parece un concepto claro, básico, sin embargo, vale la pena considerar las siguientes preguntas: ¿Qué es el tamaño del sector salud?, ¿Cómo se determina? ¿Cómo se puede observar?
\par
No es difícil reconocer que el tamaño se puede definir cuando se le considera como sector económico o por su cobertura de servicios, entre otras muchas posibilidades. El hecho, es que todo atributo requiere otro nivel con mayor especificidad para definir como se habrá de medir, observar y registrar el fenómeno. Este nuevo nivel de especificidad se conoce con el nombre de \azulcursiva{Variable}. 
\begin{definition}
\azulcursiva{Una Variable es una codificación \textbf{numérica} específica de un atributo que puede ser registrada cada vez que se observa el fenómeno bajo estudio.
}
\end{definition}
\par
Un atributo puede ser registrado a través de una o más variables. En cualquier caso, es conveniente insistir en que, por definición, toda variable es \textit{numérica}. Esta propiedad permite que los métodos estadísticos sean generales y no particulares de cada investigación.
\par
Ahora bien, todas las variables son codificaciones numéricas, pero no todas son del mismo tipo. Por ejemplo, si interesa el tamaño del sector salud en términos de su cobertura, una variable podría ser el número de hospitales de tercer nivel en el país; otra posibilidad es el porcentaje de localidades con menos de mil habitantes que cuentan con una clínica de primera atención y una más podría ser una variable que tome el valor uno si el sarampión está erradicado o cero si aun se presentan casos. En todos estos ejemplos el resultado es una codificación numérica pero no todas tienen la misma naturaleza. 
\par
El número de hospitales es una codificación numérica de naturaleza realmente cuantitativa discreta. El porcentaje de localidades es también cuantitativa pero puede considerarse continua. En ambos casos se pueden analizar los valores que producen estas variables utilizando las operaciones aritméticas habituales. 
\par
Por su parte, la variable que se relaciona con el sarampión, si bien emplea códigos numéricos, no representa una cantidad sino una cualidad y no es susceptible de tratarse libremente con las operaciones aritméticas. Los métodos estadísticos reconocen estas diferencias entre las variables y las incorporan en el análisis.
\par
Vale la pena insistir en que una variable no esta definida sino hasta que establece, por completo, la codificación con que registra el atributo de interés. Como ilustración, cuando se trata de la talla de las personas, la \azulcursiva{Estatura} no es una variable hasta que no se define como y con que unidades de habrá de registrar (en metros y centímetros, en pies y pulgadas, o como un código 0 si no supera un metro y medio, y un código 1 en caso contrario, por ejemplo).
\par
Finalmente, se tiene la idea de los Datos que siempre se asocian al proceso de observación de los fenómenos. 
\begin{definition}
\azulcursiva{Un Dato es el resultado específico de la observación de una variable concreta en un caso particular.} 
\end{definition}
\par
De esta forma se establece la secuencia jerárquica:  \azulcursiva{$ Atributos \implies Variables \implies  Datos $}, que se representa en la Figura 1 y resulta vital para el diseño de una investigación, en particular si la investigación es estadística. Es conveniente recordar que la materia de estudio de la Estadística son los fenómenos cuyos resultados presentan variabilidad. Es por esta razón que la observación repetida juega un papel fundamental en la realización de los estudios estadísticos. Si no hubiese variabilidad, una sola observación bastaría para producir la descripción deseada (y establecer la consiguiente relación causa-efecto). Como no es este el caso, el estudio necesariamente requiere la observación repetida que produce una colección de datos (con variabilidad).

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\textwidth]{Figuras/Figura1.El proceso de Observación.jpg}
    \caption{El proceso de Observación}
    \label{fig: El proceso de Observacion}
\end{figure}
En lo sucesivo, emplearemos letras mayúsculas para referirnos en forma genérica a las variables en el estudio $(W,X,...,Z)$ y, en correspondencia, letras minúsculas para los datos que se obtienen al observar cada una de esas variables $(w,x,...,z)$.Así, si se tiene un atributo A, que se registra a través de la variable X, los datos que se obtienen a partir de la observación (repetida n veces) resultan 
$x_{1},x_{2},...,x_{n}$.
\par
Volviendo al objetivo del estudio, este es \azulcursiva{describir} el fenómeno. Para ello se produce la observación y con el resultado, los datos, se aborda la descripción, tal como se ilustra en la Figura \ref{fig: El ciclo Observación-Descripción}
\par
\begin{figure}[H]
    \centering
    \includegraphics[width=.9\textwidth]{Figuras/Figura2.El ciclo Observación-Descripcion.jpg}
    \caption{El ciclo Observación-Descripción}
    \label{fig: El ciclo Observación-Descripción}
\end{figure}
Una vez que el fenómeno se observa a través del atributo $A$, la descripción se ha de producir necesariamente en referencia a ese atributo. En la misma línea, cuando $A$ se registra mediante la variable $X$, la tarea de la Estadística es ahora describir esta variable. Finalmente, la información disponible sobre $X$ es la que proveen los datos, así que la descripción del fenómeno, en última instancia, se produce al describir la colección de datos $x_{1},x_{2},...,x_{n}$.
\par
Puesto que los datos no son todos iguales, pudiera pensarse que su mejor descripción es simplemente el listado completo de todos ellos. Esta es una posibilidad que puede tener sentido cuando la cantidad de datos es pequeña, pero si $n$ crece, el conjunto de datos eventualmente alcanzara un volumen tal que un examen de los datos, uno por uno, será complicado, con frecuencia, inútil y en un extremo, imposible. 
\par
La solución general que los estadísticos han propuesto para este problema es natural y consiste en la producción de \azulcursiva{resúmenes} (gráficos o numéricos) de los datos.
\par
Por supuesto, esos resúmenes deben capturar las características relevantes para el investigador. Así  que, tres ideas que se deben mantener presentes a lo largo de todo el texto son las siguientes:
\begin{itemize}
\item \azulcursiva{Las descripciones que produce la Estadística se refieren a los fenómenos de interés, pero se concretan a través de la descripción de los datos, fruto  la observación.}
\item \azulcursiva{Las técnicas y métodos estadísticos son especialmente útiles cuando el volumen de datos es  grande.}
\item \azulcursiva{La Estadística describe los conjuntos de datos a través de resúmenes.}
\par
\end{itemize}
En particular, en relación con el tercer apartado, vale la pena anotar que cuando se estudia la disciplina Estadística, como lo haremos en este curso, lo que en realidad se investiga son las ideas, los conceptos, los métodos y las técnicas para producir los mejores resúmenes de los datos, los resúmenes óptimos. Este apunte es relevante porque la Estadística es una disciplina científica y, en consecuencia, el conocimiento que produce debe tener esta calidad, \azulcursiva{óptima}.
\par
En cada estudio particular es posible que se puedan producir distintos resúmenes; unos buenos, otros mediocres y algunos otros, muy pobres. Si, como ejemplo, es de interés la talla (atributo) de los 560 estudiantes varones de licenciatura en una universidad, y la variable que se registra en cada uno de ellos es la estatura, en metros y centímetros, es razonable suponer que los 560 datos presentarán variabilidad (se trata de un estudio estadístico). También es claro que un resumen posible de estos datos, es la media (la estatura promedio). Sin embargo, no es  automático que este resumen sea el mejor. Si el estudio se realiza para decidir sobre la pertinencia de organizar un equipo de baloncesto que represente al programa, seguramente, un resumen mas útil sería la fracción de estudiantes que superan el 1.90 metros de estatura.
\par
El objetivo de la Estadística en esta materia es producir, en cada caso, los resúmenes óptimos. Por supuesto, para asegurar que un resumen es el mejor, será indispensable establecer criterios para evaluarlos y, además, será necesario tomar en cuenta, tanto la naturaleza de las variables que se analizan, como los objetivos del estudio. En general, se puede afirmar que los mejores resúmenes serán los que, en el menor volumen, conserven toda la información relevante. Los que tengan la mayor capacidad de síntesis, sin perder información valiosa. Utilizando los términos técnicos que se precisaran mas adelante, los mejores resúmenes son los \azulcursiva{suficientes} y \azulcursiva{minimales}.
\section{Dos Vertientes de la Estadística}
\par
Como ya se estableció, toda la Estadística es descriptiva y la descripción que produce se realiza a partir de los datos. Ahora bien, existen dos casos que es extraordinariamente importante distinguir. 
\par
Por una parte, en algunos estudios es posible acceder a todos los datos que el fenómeno puede producir. En el lenguaje estadístico, se dice que se cuenta con un Censo o que se observó la Población de Interés en su totalidad. 
\par
Los ejemplos mas obvios de este tipo son precisamente los Censos de Población y los Censos Económicos que periódicamente se realizan en las diferentes naciones y por medio de los cuales, en los primeros, se registra el sexo, la edad y otras características de \azulcursiva{todos} y cada uno de los habitantes del país. En los censos económicos, por su parte, se obtiene información de \azulcursiva{todas} las unidades económicas del país sobre su giro, su producción y sus ventas, entre otros aspectos. Los censos de población y económicos son estudios enormes, pero existen otros ejemplos de alcance mucho menor, en los que no solo es posible sino forzoso realizar un censo. Cuando una empresa está evaluando sus pasivos contingentes, en particular sus pasivos laborales, como parte del estudio recabará información de \azulcursiva{todos} sus empleados. Así, registrará las edades, los salarios y los años de antigÃŒedad, para todos sus empleados, sin importar su número.
\par
En cualquier caso, cuando se lleva a cabo un censo, y por tanto, se tiene la información completa sobre el fenómeno de interés, la descripción que se produce de los datos equivale a una \azulcursiva{descripción exacta} del fenómeno. Por supuesto, habrá sido necesario elegir los resúmenes óptimos, atendiendo a los objetivos del estudio y a la naturaleza de las variables que se registran pero, una vez con ellos, la descripción que se produce es un reflejo fiel del fenómeno y el estudio finaliza. 
\par
Las técnicas estadísticas que se aplican en estas circunstancias constituyen la primera de las dos vertientes a las que se refiere el título de esta sección, se agrupan bajo la denominación general de Análisis Exploratorio de Datos y se presentan en una diversidad de textos. Una referencia clásica es el libro de Tukey (1977), si bien existen libros más recientes que hacen uso de los recursos modernos de cómputo. El tema es de gran interés y de suma utilidad. Sin embargo, no es el objeto de estudio en este curso.
\vspace{1cm}
\begin{center}
\fcolorbox{red}{white}{\begin{minipage}{13cm}
\centering
\azulcursiva{\large Análisis Exploratorio de Datos$\implies$Descripción Exacta}
\end{minipage}}
\end{center}
\vspace{1cm}
\par
El otro tipo de estudio, cuando se trata de un análisis estadístico, que entraña aun más retos y del que nos vamos a ocupar en lo que resta de este curso, ocurre cuando no es posible recolectar todos los datos que puede producir el fenómeno de interés. Las causas que impiden observar todos los datos pueden ser de lo mas variadas. El costo puede ser un factor.
\par
Imagine que un partido político esta interesado en conocer las preferencias electorales en una localidad como la Ciudad de México, que en 2018 contaba con mas de 7.5 millones de electores registrados. Convocar a todos esos ciudadanos para que expresen sus preferencias, equivale a organizar una votación completa, un ejercicio extremadamente costoso. Lo habitual, en estos casos, es entrevistar a unos pocos centenares de electores y utilizar sus opiniones como un referente de lo que opina la totalidad de los electores. En este punto, vale la pena introducir otra definición.
\begin{definition}
\azulcursiva{Cuando sólo se tiene acceso a una fracción de los datos que el fenómeno puede generar, cuando solamente se observa una parte de la Población de Interés, a la fracción observada se le denomina \textbf{Muestra}.
}
\end{definition}
Otro ejemplo, en el terreno farmacéutico, en el que no tiene sentido considerar un censo, se presenta cuando se ensayan medicamentos nuevos. Suponga que se tiene una nuevo tratamiento contra la influenza y se desea valorar el efecto que tiene en personas adultas, de origen hispánico, con síntomas claros del padecimiento, que no presenten otra enfermedad concurrente.
\par
El problema es que, al momento del estudio (sea este cual sea), el grupo completo de los individuos que forman la población de interés no se encuentra al alcance del estudio. No solamente es inviable incluir a todas las personas que tengan ese perfil en el país (o en el planeta), sino que habrá personas que cumplan los requisitos en el futuro pero que, en ese momento, todavía no existen. En estas condiciones, un censo es imposible. Una alternativa es probar el tratamiento en un grupo reducido de personas (la \azulcursiva{muestra}), con la idea de que los resultados se puedan hacer extensivos a todos los pacientes de su tipo, actuales o futuros. 
\par
En el área de control de calidad también se presentan problemas de este tipo. En términos simples, el control de calidad es un proceso que se aplica a un producto con el objetivo de establecer si cumple con una serie de especificaciones (de calidad). 
\par
Suponga que en una industria se fabrican tabletas electrónicas que se comercializan con la garanta de que, bajo ciertas condiciones ambientales, se mantienen operando sin fallas, por al menos 100 horas continuas. En este caso, el control de calidad tiene, entre otros objetivos, la misión de verificar que el porcentaje de productos que no alcanzan ese estándar es tan pequeño que las pérdidas por incumplimiento de la garanta representan un riesgo aceptable. Para determinar si una tableta cumple con la norma, es necesario mantenerla en funcionamiento, en las condiciones prescritas, hasta que falle o alcance el límite de las 100 horas.
\par
El asunto es que, sea cual sea el resultado, esa tableta, una vez que se sometió a la prueba de calidad, ya no es comercializable. La prueba es \azulcursiva{destructiva} y, por supuesto, en ningún caso un fabricante estará dispuesto a destruir la \azulcursiva{totalidad} de su producción, ni siquiera si cree que, al final, todas las tabletas habrán superado la prueba satisfactoriamente. 
\par
En la práctica, las pruebas de control de calidad son indispensables y para obtener datos, la prueba se aplica solamente a una muy pequeña fracción de la producción completa (de nuevo una muestra) y los resultados se utilizan para describir a la totalidad de las tabletas producidas o por producir.
\par
Otros casos, donde la observación destructiva ocurre cotidianamente, se presentan en Medicina. Como ilustración, considere los exámenes de sangre. Cuando un médico le solicita a una persona que se practique una prueba de química sanguínea, lo que busca son datos que le permitan describir el funcionamiento del organismo de su paciente. Los resultados de un examen de este tipo suelen incluir información sobre la densidad de eritrocitos, leucocitos y plaquetas. Con frecuencia reportan también concentraciones de glucosa, lípidos y triglicéridos entre otros elementos. 
\par
Como sabe todo aquel que ha pasado por estas experiencia, para obtener estas mediciones no se realiza un censo. No se extrae toda la sangre (entre cuatro y cinco litros) del paciente sino que se obtiene solo una muestra (habitualmente de no mas de unos pocos mililitros.
\par
En resumen, entre las causas que impiden la realización de los censos se encuentran su costo, la inaccesibilidad de la población de interés y la destrucción de los elementos que son objeto de la observación.
\par
Podríamos continuar con una larga lista de estudios en los que la idea de realizar un censo para describir el fenómeno de interés es inaceptable. Sin embargo, con los ejemplos a la mano ya es posible concluir que, aun cuando no sea posible un censo, la necesidad de describir los fenómenos inciertos correspondientes permanece y, por tanto, el reto de la Estadística en este caso es diseñar métodos y técnicas para producir la descripción que interesa con la \azulcursiva{información parcial e incompleta} que ofrece una muestra. En otras palabras, la disciplina debe contar con la capacidad para describir fenómenos en lo general, a partir de colecciones de datos parciales. Las técnicas, métodos y procedimientos que se ocupan de este problema se agrupan bajo la denominación de \azulcursiva{Inferencia Estadística}, cuyo estudio será nuestro centro de atención este semestre.
\par
Antes de abordar sus procedimientos específicos, vale la pena establecer algunas ideas generales que caracterizan a la Inferencia Estadística. Como en cualquier otro caso, cuando se han definido los resúmenes óptimos para el problema particular, estos pueden ser utilizados para describir la muestra a la que se tiene acceso y, de acuerdo con el Análisis Exploratorio de Datos, la descripción que producen será \azulcursiva{exacta en tanto se refiera a la muestra}.
\par
El objetivo, sin embargo, no es describir la muestra sino describir la población completa (el fenómeno en su totalidad) a partir de la información comprendida en la muestra. Así que el siguiente paso es fundamental y consiste en extrapolar la descripción de la muestra para describir  la población completa. En este punto es evidente la mas importante diferencia de la Inferencia Estadística con el Análisis Exploratorio de Datos: Sin importar la forma en que se haya seleccionado la muestra ni el mecanismo concreto de extrapolación, la descripción de la población es, en el mejor de los casos, aproximada. \par
\begin{center}
\fcolorbox{red}{white}{\begin{minipage}{13cm}
\centering
\azulcursiva{\large Inferencia Estadística$\implies$Descripción \textbf{Aproximada}}
\end{minipage}}
\end{center}
\par
Por ejemplo, en el caso del estudio de preferencias realizado por el partido político, si el 27\% de los electores en la muestra declara que votaría por el partido, ese porcentaje describe \azulcursiva{exactamente} lo que manifiestan las personas en la muestra. Sin embargo, si ese porcentaje se utiliza para describir las preferencias de \azulcursiva{todos} los electores, es necesario reconocer que, aun si la muestra refleja razonablemente lo que ocurre en el conjunto de todos los electores, solamente se puede decir que \azulcursiva{aproximadamente} el 27\% de los electores en la población piensa votar por el partido.
\par
De esta manera, la diferencia fundamental entre las dos vertientes de la Estadística (Análisis Exploratorio de Datos e Inferencia Estadística) radica en el hecho de que la descripción que producen de los fenómenos es, en el primer caso, exacta y en el segundo, solamente aproximada.
\par
Y el asunto no para ahí. Reconocer que la descripción es aproximada es de la mayor importancia pero resulta irrelevante en la práctica, si no es posible contar con una idea razonable y útil del grado de aproximación que implica. 
\par
¿Qué significa la afirmación de que aproximadamente el 27\% de los electores piensa votar por el partido? En la población completa, ese porcentaje ¿Se encuentra entre 25\% y 29\%? ¿Entre 4\% y 50\%? y, de ser correcto alguno de estos casos ¿Qué tan seguros podemos estar de ello?
\par
Así, un primer reto específico de la Inferencia consiste en medir, en cuantificar, el grado de aproximación que tienen las descripciones que produce. Una idea que aparece de forma natural en relación con este desafío, es la noción, \textit{correcta}, de que la aproximación será mejor en la medida en que la muestra refleje mejor lo que ocurre en la población. De hecho, si la muestra fuese una reproducción exacta, a pequeña escala, de la población completa, entonces la descripción resultaría también exacta. A una muestra con esa propiedad ideal se le denomina \azulcursiva{representativa}.
\par
De este razonamiento impecable, surge una quimera. Todo investigador enfrentado a un problema de inferencia quisiera contar con muestras representativas. Y es un desvarío, porque la única forma de asegurar que una muestra es representativa, requiere el conocimiento de la población completa, en cuyo caso el problema de inferencia simplemente se desvanecería. Así que las muestras representativas, deseables sin duda, \azulcursiva{no se pueden garantizar en la práctica} y anunciar que un estudio se realiza con una muestra representativa es un despropósito.
\par
No obstante, durante algún tiempo la selección de la muestra se llevaba a cabo con la asesoría de expertos, conocedores de algunos rasgos generales de la población de interés que, con esa información, seleccionaban muestras, con aspiraciones de representatividad.
\par
Esta práctica tenía, y tiene, al menos dos inconvenientes. Por una parte, no garantiza la reproducibilidad del estudio. Dos expertos pueden diseñar muestras distintas y no hay forma de establecer si las diferencias entre los resultados correspondientes son reconciliables. El segundo inconveniente, que se relaciona con el primero, es que con una muestra \textit{experta} sigue sin respuesta el problema de medir el grado de aproximación de la inferencia que se produce. 
\par
Por supuesto, existen ejemplos donde un experto seleccionó una muestra que fue tratada como representativa y condujo a resultados con un nivel de aproximación (verificada \textit{a posteriori}) extraordinario. Desafortunadamente, los casos en que se puede realizar esa verificación \textit{postmortem}, son la excepción y, peor aun, lo deseable es contar con una buena opinión acerca de la calidad de la muestra antes de realizar el estudio, no después. 
De cualquier manera, recurrir a un experto por eficaz y confiable que sea, no es una alternativa recomendable para acumular los datos que darán lugar a una pieza de conocimiento científico. Depender de una persona específica para producir resultados, además de contravenir la idea de reproducibilidad que ya se ha mencionado, introduce un factor de fragilidad indeseable.
\section{El Muestreo Probabilístico}
Para fortuna de los usuarios de los métodos estadísticos, los retos que, de origen, entraña la inferencia fueron razonablemente resueltos en las primeras décadas del siglo XX y, más aun, el procedimiento propuesto provee una solución, simultánea, al tema de la selección de la muestra y al de la medición del grado de la aproximación en la inferencia.
\par
Es conveniente reconocer que, a lo largo del tiempo, una gran cantidad de estadísticos contribuyeron a la solución definitiva y que, en alguna forma, el asunto quedo zanjado en 1934, tras la publicación del artículo de Jerzy Neyman que se considera un hito en el tema. El título resulta evocador cuando se conoce el problema: \azulcursiva{On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection}.
\par
La solución consiste en seleccionar la muestra por sorteo o, en un lenguaje mas técnico, en utilizar un procedimiento de \azulcursiva{selección probabilística de la muestra}. Las consecuencias de esta idea, tanto conceptuales, técnicas y prácticas, son extraordinarias y, en buena parte, serán nuestro principal objeto de estudio. La versión mas sencilla del muestreo por sorteo se puede describir muy fácilmente.
\par
Suponga que el conjunto completo de todas las observaciones que puede producir un fenómeno es finito, pero muy grande, de tal forma que hace inviable la realización de un censo. Si de ese conjunto que, digamos, consta de $N$ elementos, se desea seleccionar una muestra de tamaño $n$ ($n << N$) se procede como sigue:
\begin{enumerate}
\item Se identifica a cada elemento en la población con una, y solo una, etiqueta en el conjunto $\{1, 2,...,N\}$
\item Se depositan las $N$ etiquetas en una urna.
\item Después de asegurar que han sido bien mezcladas, se extrae una etiqueta, que identifica al primer elemento que formará parte de la muestra.
\item Se procede de igual manera con las $N-1$ etiquetas restantes en la urna para identificar al segundo integrante de la muestra.
\item Se repite el procedimiento hasta identificar a todos los elementos que formaran la muestra de tamaño $n$.
\end{enumerate}
\par
El mecanismo equivale a un experimento aleatorio a través del que se obtiene una muestra de tamaño $n$ sin reemplazo, de elementos en la población. La información de interés se obtiene al realizar la observación de la variable $X$, en cada uno de los elementos seleccionados, lo que dará origen a la colección de datos $x_1,x_2,...,x_n$.
\par
Una primera consideración, que conceptualmente es de la mayor importancia, es que bajo este esquema no es necesario suponer que el fenómeno bajo estudio produce sus observaciones en forma aleatoria. Los datos presentan variabilidad, como ya se ha discutido, pero la aleatoriedad es debida a, y garantizada por, el mecanismo de selección de la muestra. De hecho, a una muestra de este tipo se le denomina una \azulcursiva{muestra aleatoria}. El resultado es fundamental puesto que la naturaleza aleatoria de las observaciones está garantizada, es independiente del fenómeno y el empleo de la probabilidad para describirlas es inmediato.
\par
Un segundo aspecto no menos relevante tiene que ver con la relación que guardan las muestras aleatorias con la idea de representatividad de la muestra. Como ya hemos comentado, no es posible afirmar que una muestra es representativa a menos que se conozca la población completa.
\par
Ni siquiera en el caso del muestreo probabilístico se puede decir que las muestras que produce sean representativas. Sin embargo, con este algoritmo cada elemento de la población tiene la misma probabilidad de ser incluido en la muestra y, como consecuencia, los valores de $X$ mas frecuentes en la población serán necesariamente los mas probables en la muestra. Es decir, \azulcursiva{lo más probable} es que la muestra resulte parecida a la población. En otras palabras, no hay garantía, pero lo más probable es que la muestra se aproxime a la quimera de la representatividad.
\par
Por otra parte, el mecanismo es reproducible y exógeno; no existe la posibilidad de un sesgo deliberado en la selección de la muestra. Ya que en cada paso de la selección de la muestra se realiza un experimento aleatorio, el dato correspondiente al \textit{i-ésimo} paso,$x_i$, se puede considerar la realización de una variable aleatoria $X_i$ para $i$= 1,2,âŠ,$n$. Más aun, puesto que $n$ cumple la condición $n << N$, es razonable suponer que el contenido de la urna no cambia tras cada extracción y entonces, las variables $X_1,X_2,...,X_n$ son  independientes e idénticamente distribuidas. 
\par
Equivalentemente, se puede suponer que los datos, $\underline{x}_{(n)}=\{x_1,x_2,...,x_n\}$ son el resultado de observar $n$ realizaciones independientes $\underline{X}_{(n)}=\{X_1,X_2,...,X_n\}$ de una misma variable aleatoria $X$.
\par
En efecto, a partir de ahora se adoptará la siguiente
\begin{definition}
\azulcursiva{Se dice que $\underline{X}_{(n)}=\{X_1,X_2,...,X_n\}$ es una muestra aleatoria (m.a.) de la variable aleatoria $X$ si constituye una colección de variables aleatorias independientes 
e idénticamente distribuidas (i.i.d) de acuerdo a la función de la distribución de $X$}.
\end{definition}
\par
Esta formulación tiene efectos muy importantes en el proceso de inferencia. Originalmente, la investigación tiene como propósito describir el fenómeno de interés. En cuanto se fija el atributo a través del cual se va a registrar el fenómeno, el objetivo es describir el atributo. En el siguiente nivel, una vez que se define la variable que precisa el atributo, la descripción que se persigue es la de esa variable. Si fuese posible realizar un censo, se conocerían todos los datos que la observación de la variable puede producir y con ellos se alcanzara la meta. 
\par
En nuestro caso, de muestreo probabilístico, solamente tenemos acceso a una muestra aleatoria de la variable aleatoria $X$. Así, entonces, a partir de la información muestral, es necesario producir una descripción (aproximada) de todos los datos que $X$ puede producir.
\par
Se busca una descripción de los valores distintos que puede tomar la variable y de la frecuencia con que se presentan en la población. En términos técnicos, interesa describir el soporte de $X$ y la probabilidad con que produce cada uno de los valores en ese soporte. En resumen, el objetivo del estudio se reduce a describir la \azulcursiva{Función de Distribución $F(X)$}
\par
El objetivo es mas específico, pero no mas simple; determinar la distribución de una variable aleatoria a partir de un puñado de observaciones es un problema complejo. El conjunto $\mathcal{F}$ de todas las funciones de distribución cuenta no solo con un numero infinito de elementos, sino que es de dimensión infinita. Se trata de la colección de todas funciones que satisfacen las condiciones:
\vspace{5mm}
\begin{center}
\begin{minipage}{.5\textwidth}
\begin{itemize}
    \item $F(X)$ monótona no decreciente.
    \item $F(X) \xrightarrow[]{}0$  cuando $x\xrightarrow[]{} -\infty$.
    \item $F(X) \xrightarrow[]{}1$  cuando $x\xrightarrow[]{} \infty$.
    \item $F(X)$ continua por la derecha.
\end{itemize}
\end{minipage}
\end{center}
\vspace{5mm}
\par
Seleccionar una función $F$ en $\mathcal{F}$, a partir de los datos en la muestra, que constituya la descripción óptima del fenómeno de interés representa un reto formidable. Además de requerir un criterio de optimalidad, que en cualquier caso habría que proponer, la búsqueda del óptimo en un espacio de dimensión infinita dista de ser trivial.
\section{Inferencia Estadística Paramétrica}
Una alternativa que reduce conceptual y cualitativamente esta dificultad, consiste en considerar, no el conjunto de todas las funciones de distribución, sino un subconjunto suficientemente grande como para que la descripción que se obtenga sea razonable, pero tal que la optimización se pueda llevar a cabo con herramientas matemáticas relativamente simples.
\par
La idea es recurrir a una familia paramétrica $\mathcal{F}_{\theta}$de modelos de probabilidad. En este caso, se considera que la función de distribución que interesa,$F(X)$, pertenece a un conjunto de la forma
\begin{center}
$\mathcal{F}_{\theta} = \{ F(x) \mid F(x)=F(x;\theta), \theta \in \Theta \}$
\end{center}
donde, para cada valor de $\theta$ fijo, la distribución $F(x)=(x;\theta)$ es totalmente conocida. $\theta$ se conoce como el parámetro de la familia y es un índice que identifica de manera inequívoca a cada elemento en $\mathcal{F}_{\theta}$.
\par
En esta formulación, la determinación de la función de distribución óptima se reduce a la determinación del parámetro óptimo. Si el espacio paramétrico $\Theta$ es un subconjunto del espacio $\mathbb{R}^k$ con $k$ fijo, el problema es uno de optimización en $\mathbb{R}^k$ que se puede abordar, una vez seleccionado el criterio de comparación, con los recursos habituales del cálculo.
\par
En la literatura es posible encontrar una variedad de familias paramétricas de funciones de distribución, que pueden utilizarse en el proceso de inferencia que aquí se discute. También es posible definir nuevas familias paramétricas con características que se consideren propicias para describir un tipo específico de datos. En todo caso, es importante insistir en que la elección de una familia paramétrica se produce con el objetivo de simplificar la tarea de describir el fenómeno de interés y necesariamente, implica el empleo de un modelo particular, con restricciones y limitantes que debe elegirse con cuidado.
\par
Para seleccionar la familia paramétrica se considera la definición de la variable (¿Cuál es su soporte?, ¿Toma valores en un conjunto finito, numerable o potencialmente en un continuo?), el marco de referencia (¿La teoría o el conocimiento previo sugieren algún comportamiento particular de los datos, como simetrías?) y, por supuesto, el análisis estadístico de muestras preliminares o provenientes de estudios similares.
\par
En lo que resta de este curso se partirá del supuesto de que la familia paramétrica $\mathcal{F}_{\theta}$ ya ha sido seleccionada y nos concentraremos en el problema remanente de producir inferencias sobre el valor del parámetro desconocido. Esta es el tema de que nos ocuparemos en los siguientes meses: La \azulcursiva{Inferencia Estadística Paramétrica}.
\par
El material que sigue tiene la estructura habitual de los libros de Inferencia Estadística Paramétrica y se desarrollará de acuerdo a una escuela de pensamiento estadístico particular cuyas bases se desarrollaron en los primeros treinta años del siglo XX. 
\par
Se trata de la escuela \azulcursiva{Frecuentista} que debe su nombre a la interpretación que utiliza de la probabilidad. Es un enfoque muy exitoso y dominó por completo el panorama estadístico por mas de 50 años. Sigue siendo uno de los dos paradigmas mas importantes en la Estadística si bien ahora comparte el escenario con el Análisis Estadístico Bayesiano que se revisa en otro curso.
\par
En cualquier caso, las ideas, conceptos, técnicas y métodos que se revisarán en el resto del curso tienen, como ya se ha indicado, el propósito de revisar el tema de la inferencia sobre el parámetro $\theta$ Si se tiene una buena idea (aproximada) del valor del parámetro del modelo, se tendrá una idea (aproximada también) de la función de distribución de la variable que dio origen a los datos. En términos de inferencia interesa, en particular, proveer respuestas a las siguientes preguntas: 
\vspace{5mm}
\begin{center}
\begin{minipage}{.6\textwidth}
\begin{enumerate}
    \item ¿Cuál es el valor de $\theta$?
    \item ¿Por dónde se localiza el parámetro $\theta$?
    \item Dada una idea preconcebida sobre $\theta$ 
	  ¿Los datos son compatibles con ella?
\end{enumerate}
\end{minipage}
\end{center}
\par
\vspace{5mm}
Para dar respuesta a la primera pregunta, nos ocuparemos de las técnicas de \azulcursiva{Estimación Puntual}; para la segunda recurriremos a las técnicas de \azulcursiva{Estimación por Regiones} y la tercera la abordaremos en el apartado de \azulcursiva{Contraste o Prueba de Hipótesis}. Iniciaremos con el apartado de Estimación Puntual.
\section{Ejercicios}
*colocar lista de ejercicios aquí*





























































